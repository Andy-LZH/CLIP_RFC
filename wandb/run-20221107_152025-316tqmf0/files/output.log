
  7%|███████████▍                                                                                                                                               | 5/68 [00:01<00:14,  4.23it/s]
loss:  tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.8487, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.9459, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(1.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)

 21%|███████████████████████████████▋                                                                                                                          | 14/68 [00:03<00:12,  4.25it/s]
loss:  tensor(0.8492, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.6215, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(1.7644, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(1.0877, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.7601, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(1.0901, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.6606, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5073, device='cuda:0', grad_fn=<NllLossBackward0>)

 34%|████████████████████████████████████████████████████                                                                                                      | 23/68 [00:05<00:10,  4.49it/s]
loss:  tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5712, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5512, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.7320, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4412, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5500, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward0>)

 47%|████████████████████████████████████████████████████████████████████████▍                                                                                 | 32/68 [00:07<00:07,  4.56it/s]
loss:  tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5037, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4693, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5104, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5295, device='cuda:0', grad_fn=<NllLossBackward0>)

 60%|████████████████████████████████████████████████████████████████████████████████████████████▊                                                             | 41/68 [00:09<00:06,  4.17it/s]
loss:  tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4762, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4519, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)

 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                        | 50/68 [00:11<00:03,  4.52it/s]
loss:  tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5759, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5371, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5299, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4858, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5353, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4820, device='cuda:0', grad_fn=<NllLossBackward0>)

 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 59/68 [00:13<00:02,  4.35it/s]
loss:  tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.5304, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4788, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4552, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.3624, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4302, device='cuda:0', grad_fn=<NllLossBackward0>)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68/68 [00:15<00:00,  4.38it/s]
  0%|                                                                                                                                                                  | 0/977 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "train_MHIST.py", line 212, in <module>
    train(train_data, test_data, model_backbone, max_epoch)
  File "train_MHIST.py", line 178, in train
    prob = y_pred.cpu().numpy()
RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.
loss:  tensor(0.4862, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4186, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.6008, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4274, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.3518, device='cuda:0', grad_fn=<NllLossBackward0>)
loss:  tensor(0.4462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training loss: 0.018454616316433612, accuracy: 0.7126436781609196